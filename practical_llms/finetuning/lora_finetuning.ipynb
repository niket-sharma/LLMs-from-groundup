{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning Tutorial\n",
    "\n",
    "This notebook demonstrates **Parameter-Efficient Fine-Tuning** with LoRA (Low-Rank Adaptation).\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Basic LoRA**: Train only 0.1-1% of parameters\n",
    "2. **QLoRA**: 4-bit quantization for large models\n",
    "3. **DPO + LoRA**: Preference alignment with minimal memory\n",
    "4. **Adapter Management**: Save, load, merge adapters\n",
    "\n",
    "## Benefits:\n",
    "- ‚úì 100x less memory than full fine-tuning\n",
    "- ‚úì 10x faster training\n",
    "- ‚úì Works on consumer GPUs (even 4GB VRAM!)\n",
    "- ‚úì Adapters are tiny (~few MB) and shareable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "Run this cell first to set up the environment and detect your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5070 Laptop GPU\n",
      "VRAM: 8.0 GB\n",
      "\n",
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"distilgpt2\"  # Small model for demos (~82M params)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU (slower but works!)\")\n",
    "\n",
    "print(\"\\n‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Auto-Configuration\n",
    "\n",
    "This function detects your GPU VRAM and sets optimal training parameters automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-configured for 8.0GB VRAM:\n",
      "  - Batch size: 2\n",
      "  - Max length: 256\n",
      "  - FP16: True\n",
      "  - Gradient checkpointing: True\n"
     ]
    }
   ],
   "source": [
    "def detect_gpu_config():\n",
    "    \"\"\"Detect GPU and return optimized configuration.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\n",
    "            'device': 'cpu',\n",
    "            'batch_size': 2,\n",
    "            'use_fp16': False,\n",
    "            'max_length': 256,\n",
    "            'gradient_checkpointing': True,\n",
    "        }\n",
    "    \n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    \n",
    "    if vram_gb < 4:\n",
    "        config = {'batch_size': 1, 'max_length': 128}\n",
    "    elif vram_gb < 8:\n",
    "        config = {'batch_size': 2, 'max_length': 256}\n",
    "    elif vram_gb < 12:\n",
    "        config = {'batch_size': 4, 'max_length': 512}\n",
    "    else:\n",
    "        config = {'batch_size': 8, 'max_length': 512}\n",
    "    \n",
    "    config.update({\n",
    "        'device': 'cuda',\n",
    "        'use_fp16': True,\n",
    "        'gradient_checkpointing': vram_gb < 12,\n",
    "    })\n",
    "    \n",
    "    print(f\"Auto-configured for {vram_gb:.1f}GB VRAM:\")\n",
    "    print(f\"  - Batch size: {config['batch_size']}\")\n",
    "    print(f\"  - Max length: {config['max_length']}\")\n",
    "    print(f\"  - FP16: {config['use_fp16']}\")\n",
    "    print(f\"  - Gradient checkpointing: {config['gradient_checkpointing']}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "GPU_CONFIG = detect_gpu_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding LoRA\n",
    "\n",
    "LoRA works by adding small trainable matrices to the model:\n",
    "\n",
    "```\n",
    "Normal fine-tuning:  Update all W (millions/billions of parameters)\n",
    "LoRA fine-tuning:    Add W' = A √ó B (only thousands of parameters!)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `W` is the original weight matrix (frozen)\n",
    "- `A` and `B` are small matrices (trainable)\n",
    "- Rank `r` controls the size: smaller = fewer parameters, larger = more capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key LoRA Parameters\n",
    "\n",
    "| Parameter | Range | What it does |\n",
    "|-----------|-------|-------------|\n",
    "| **r** (rank) | 4-64 | Higher = more capacity, more params |\n",
    "| **lora_alpha** | 16-32 | Scaling factor (usually 2√ór) |\n",
    "| **target_modules** | varies | Which layers to adapt |\n",
    "| **lora_dropout** | 0.0-0.1 | Regularization |\n",
    "\n",
    "For a **7B model**:\n",
    "- Full fine-tuning: 7B parameters\n",
    "- LoRA (r=8): ~8M parameters (0.1%)\n",
    "- LoRA (r=64): ~67M parameters (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Basic LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded: 81,912,576 parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if GPU_CONFIG['use_fp16'] else torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LoRA Applied:\n",
      "  Total params: 82,060,032\n",
      "  Trainable: 147,456 (0.18%)\n",
      "  Frozen: 81,912,576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niket/ai/LLMs-from-groundup/venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                        # Rank (try 4, 8, 16, 32)\n",
    "    lora_alpha=16,              # Scaling (usually 2√ór)\n",
    "    target_modules=[\"c_attn\"],  # GPT-2 attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# FIX: Add these 3 lines for gradient checkpointing compatibility\n",
    "if GPU_CONFIG['gradient_checkpointing']:\n",
    "    lora_model.enable_input_require_grads()\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in lora_model.parameters())\n",
    "\n",
    "print(f\"\\nüìä LoRA Applied:\")\n",
    "print(f\"  Total params: {total:,}\")\n",
    "print(f\"  Trainable: {trainable:,} ({100 * trainable / total:.2f}%)\")\n",
    "print(f\"  Frozen: {total - trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4cd6b3fae041fbac3a920df2b80ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset prepared:\n",
      "  Train: 108 examples\n",
      "  Test: 12 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create sample Q&A dataset\n",
    "data = [\n",
    "    {\"text\": \"Question: What is Python?\\nAnswer: Python is a high-level programming language known for its simplicity and readability.\"},\n",
    "    {\"text\": \"Question: Explain machine learning.\\nAnswer: Machine learning is a branch of AI where computers learn patterns from data.\"},\n",
    "    {\"text\": \"Question: What is an API?\\nAnswer: An API is a set of protocols that allows different software applications to communicate.\"},\n",
    "    {\"text\": \"Question: What is Docker?\\nAnswer: Docker is a platform for developing and running applications in isolated containers.\"},\n",
    "] * 30  # 120 examples\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=GPU_CONFIG['max_length'],\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(f\"‚úì Dataset prepared:\")\n",
    "print(f\"  Train: {len(split_dataset['train'])} examples\")\n",
    "print(f\"  Test: {len(split_dataset['test'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train with LoRA\n",
    "\n",
    "**Note:** This will actually train the model! Adjust `num_train_epochs` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_983510/2459128134.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting LoRA training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Training complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    num_train_epochs=1,  # Increase for better results\n",
    "    per_device_train_batch_size=GPU_CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=GPU_CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,  # LoRA can use higher LR\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=GPU_CONFIG['use_fp16'],\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=GPU_CONFIG['gradient_checkpointing'],\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting LoRA training...\\n\")\n",
    "trainer.train()\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save LoRA Adapters\n",
    "\n",
    "**Important:** This saves only the LoRA weights (~few MB), not the full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA adapters saved to ./my_lora_adapters/\n",
      "  (Only LoRA weights, very small!)\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "lora_model.save_pretrained(\"./my_lora_adapters\")\n",
    "tokenizer.save_pretrained(\"./my_lora_adapters\")\n",
    "\n",
    "print(\"‚úì LoRA adapters saved to ./my_lora_adapters/\")\n",
    "print(\"  (Only LoRA weights, very small!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niket/ai/LLMs-from-groundup/venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Question: What is artificial intelligence?\n",
      "Answer:\n",
      "\n",
      "Generated:\n",
      "Question: What is artificial intelligence?\n",
      "Answer: Artificial Artificial Artificial Artificial Artificial Artificial Artificial Artificial Artificial Artificial AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI AI\n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "prompt = \"Question: What is artificial intelligence?\\nAnswer:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(lora_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = lora_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nGenerated:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Loading LoRA Adapters\n",
    "\n",
    "Show how to load saved LoRA adapters onto a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LoRA adapters loaded onto base model\n",
      "\n",
      "üí° You can now use this model for inference!\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load LoRA adapters\n",
    "loaded_model = PeftModel.from_pretrained(base_model, \"./my_lora_adapters\")\n",
    "\n",
    "print(\"‚úì LoRA adapters loaded onto base model\")\n",
    "print(\"\\nüí° You can now use this model for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Adapters (For Deployment)\n",
    "\n",
    "If you want to deploy without PEFT dependency, merge the adapters into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Merged model saved to ./merged_model/\n",
      "  (Can now use without PEFT library)\n"
     ]
    }
   ],
   "source": [
    "# Merge and unload LoRA weights into base model\n",
    "merged_model = loaded_model.merge_and_unload()\n",
    "\n",
    "# Save merged model (standard HuggingFace format)\n",
    "merged_model.save_pretrained(\"./merged_model\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")\n",
    "\n",
    "print(\"‚úì Merged model saved to ./merged_model/\")\n",
    "print(\"  (Can now use without PEFT library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: DPO + LoRA (Preference Alignment)\n",
    "\n",
    "Combine **DPO** (Direct Preference Optimization) with **LoRA** for memory-efficient alignment.\n",
    "\n",
    "**Use case:** Align a model to prefer certain response styles with minimal memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DPO + LoRA?\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| DPO | No reward model needed, simpler than PPO |\n",
    "| LoRA | Memory-efficient, only 0.1-1% params |\n",
    "| Combined | Align models on consumer GPUs! |\n",
    "\n",
    "**Companies using this approach:** Many! It's a production-ready technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Preference Dataset\n",
    "\n",
    "DPO needs pairs of (chosen, rejected) responses for each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preference dataset: 45 pairs\n",
      "\n",
      "Example:\n",
      "  Prompt: Explain what Python is.\n",
      "  Chosen: Python is a high-level, interpreted programming language kno...\n",
      "  Rejected: python is a programming language i guess\n"
     ]
    }
   ],
   "source": [
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain what Python is.\",\n",
    "        \"chosen\": \"Python is a high-level, interpreted programming language known for its clear syntax and readability. It supports multiple programming paradigms and has extensive libraries.\",\n",
    "        \"rejected\": \"python is a programming language i guess\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning?\",\n",
    "        \"chosen\": \"Machine learning is a subset of artificial intelligence where systems learn patterns from data to make predictions without being explicitly programmed for each task.\",\n",
    "        \"rejected\": \"ml is when computers do stuff automatically\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain how computers work.\",\n",
    "        \"chosen\": \"A computer processes data using a CPU that executes instructions stored in memory. It takes input, processes it according to programs, and produces output.\",\n",
    "        \"rejected\": \"computers work by doing calculations fast\",\n",
    "    },\n",
    "] * 15  # 45 preference pairs\n",
    "\n",
    "dpo_dataset = Dataset.from_list(preference_data)\n",
    "\n",
    "print(f\"‚úì Preference dataset: {len(dpo_dataset)} pairs\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Prompt: {preference_data[0]['prompt']}\")\n",
    "print(f\"  Chosen: {preference_data[0]['chosen'][:60]}...\")\n",
    "print(f\"  Rejected: {preference_data[0]['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup DPO Models\n",
    "\n",
    "DPO needs:\n",
    "- **Policy model** (with LoRA) - being trained\n",
    "- **Reference model** (frozen) - for KL penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DPO models ready\n",
      "  Policy model: 294,912 trainable params (0.36%)\n",
      "  Reference model: Frozen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niket/ai/LLMs-from-groundup/venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if GPU_CONFIG['use_fp16'] else torch.float32,\n",
    ")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if GPU_CONFIG['use_fp16'] else torch.float32,\n",
    ")\n",
    "\n",
    "# Apply LoRA to policy model (NOT reference model)\n",
    "dpo_lora_config = LoraConfig(\n",
    "    r=16,  # Higher rank for alignment tasks\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "dpo_model = get_peft_model(dpo_model, dpo_lora_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in dpo_model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in dpo_model.parameters())\n",
    "\n",
    "print(f\"‚úì DPO models ready\")\n",
    "print(f\"  Policy model: {trainable:,} trainable params ({100*trainable/total:.2f}%)\")\n",
    "print(f\"  Reference model: Frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train with DPO\n",
    "\n",
    "DPO directly optimizes the policy from preference pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dc3517fdee4fc28a016d213cd76e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6b69e8620c49078f1acd9dd91299f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ec363148f0469fb56e4dfc9a2f2a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting DPO + LoRA training...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì DPO training complete!\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "# DPO configuration\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./dpo_lora_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=GPU_CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,  # Lower LR for DPO\n",
    "    beta=0.1,  # DPO temperature\n",
    "    max_length=GPU_CONFIG['max_length'],\n",
    "    max_prompt_length=GPU_CONFIG['max_length'] // 2,\n",
    "    logging_steps=5,\n",
    "    fp16=GPU_CONFIG['use_fp16'],\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting DPO + LoRA training...\\n\")\n",
    "dpo_trainer.train()\n",
    "print(\"\\n‚úì DPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save DPO Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DPO-aligned LoRA adapters saved!\n",
      "  These adapters now prefer high-quality responses.\n"
     ]
    }
   ],
   "source": [
    "dpo_model.save_pretrained(\"./dpo_lora_adapters\")\n",
    "tokenizer.save_pretrained(\"./dpo_lora_adapters\")\n",
    "\n",
    "print(\"‚úì DPO-aligned LoRA adapters saved!\")\n",
    "print(\"  These adapters now prefer high-quality responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test DPO-Aligned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain what artificial intelligence is.\n",
      "\n",
      "DPO-Aligned Response:\n",
      "Explain what artificial intelligence is. It It It It It It It It It It It It It It It It It\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare: Base model vs DPO-aligned model\n",
    "test_prompt = \"Explain what artificial intelligence is.\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(dpo_model.device)\n",
    "\n",
    "print(\"Prompt:\", test_prompt)\n",
    "print(\"\\nDPO-Aligned Response:\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = dpo_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: QLoRA (Quantized LoRA)\n",
    "\n",
    "**QLoRA = 4-bit quantization + LoRA**\n",
    "\n",
    "This enables fine-tuning **7B-70B models** on consumer GPUs!\n",
    "\n",
    "**Requirements:**\n",
    "- CUDA GPU (6GB+ VRAM recommended)\n",
    "- `pip install bitsandbytes`\n",
    "\n",
    "**Note:** This cell requires GPU. Skip if on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"‚ö†Ô∏è QLoRA requires CUDA GPU. Showing configuration only...\")\n",
    "    print(\"\"\"\n",
    "# QLoRA Configuration Example:\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Memory: Llama-2-7B\n",
    "# - Full precision: ~28 GB ‚ùå\n",
    "# - 4-bit + LoRA: ~6-8 GB ‚úì Fits on RTX 3060!\n",
    "    \"\"\")\n",
    "else:\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    if vram_gb < 6:\n",
    "        print(f\"‚ö†Ô∏è VRAM ({vram_gb:.1f}GB) too low for QLoRA demo.\")\n",
    "        print(\"Need 6GB+ for TinyLlama-1.1B with QLoRA.\")\n",
    "    else:\n",
    "        print(\"‚úì GPU detected! You can run QLoRA.\")\n",
    "        print(\"Uncomment the code below to try with TinyLlama-1.1B.\")\n",
    "        \n",
    "        # Uncomment to try:\n",
    "        # from transformers import BitsAndBytesConfig\n",
    "        # from peft import prepare_model_for_kbit_training\n",
    "        # \n",
    "        # bnb_config = BitsAndBytesConfig(\n",
    "        #     load_in_4bit=True,\n",
    "        #     bnb_4bit_quant_type=\"nf4\",\n",
    "        #     bnb_4bit_compute_dtype=torch.float16,\n",
    "        #     bnb_4bit_use_double_quant=True,\n",
    "        # )\n",
    "        # \n",
    "        # qlora_model = AutoModelForCausalLM.from_pretrained(\n",
    "        #     \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        #     quantization_config=bnb_config,\n",
    "        #     device_map=\"auto\",\n",
    "        # )\n",
    "        # \n",
    "        # qlora_model = prepare_model_for_kbit_training(qlora_model)\n",
    "        # qlora_model = get_peft_model(qlora_model, lora_config)\n",
    "        # \n",
    "        # print(f\"Memory: {qlora_model.get_memory_footprint() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What You Learned:\n",
    "\n",
    "| Technique | When to Use | Memory Savings |\n",
    "|-----------|-------------|----------------|\n",
    "| **Basic LoRA** | General fine-tuning | 100x less memory |\n",
    "| **DPO + LoRA** | Preference alignment | Same as LoRA + simple |\n",
    "| **QLoRA** | Large models (7B+) | 4x less than LoRA |\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "1. **LoRA trains only 0.1-1% of parameters** - Massive memory savings!\n",
    "2. **Adapters are tiny (~few MB)** - Easy to share and version\n",
    "3. **DPO + LoRA** - Production-ready alignment on consumer GPUs\n",
    "4. **QLoRA** - Fine-tune 7B-70B models on single GPU\n",
    "\n",
    "## Next Steps:\n",
    "\n",
    "- ‚úì Try with your own dataset from HuggingFace\n",
    "- ‚úì Experiment with different ranks (r=4, 8, 16, 32, 64)\n",
    "- ‚úì Try QLoRA with larger models (if you have 8GB+ VRAM)\n",
    "- ‚úì Load multiple adapters for multi-task models\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Fine-Tuning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
