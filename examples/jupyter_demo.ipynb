{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Small GPT Demo\n",
    "\n",
    "This notebook demonstrates how to use the Small GPT model for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from models.gpt import create_small_gpt\n",
    "from training.dataset import prepare_data, create_dataloaders, SimpleTokenizer\n",
    "from training.trainer import GPTTrainer\n",
    "from utils.inference import GPTInference\n",
    "from utils.helpers import set_seed, print_model_info, get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(42)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text data\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This sentence contains all letters of the alphabet.\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience.\n",
    "Natural language processing involves the interaction between computers and human language.\n",
    "Deep learning uses neural networks with multiple layers to model and understand complex patterns.\n",
    "Transformers have revolutionized the field of natural language processing with their attention mechanisms.\n",
    "\"\"\" * 10  # Repeat for more data\n",
    "\n",
    "# Save to file\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "with open('../data/demo_text.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset, val_dataset, tokenizer = prepare_data(\n",
    "    '../data/demo_text.txt', \n",
    "    block_size=64, \n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_dataset, val_dataset, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_creation",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'd_model': 128,\n",
    "    'n_heads': 4,\n",
    "    'n_layers': 3,\n",
    "    'd_ff': 512,\n",
    "    'max_seq_len': 128,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = create_small_gpt(config)\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = GPTTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr=1e-3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train for a few epochs\n",
    "train_losses, val_losses = trainer.train(epochs=3)\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_losses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training and Validation Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference object\n",
    "inference = GPTInference(model, tokenizer, device)\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The quick\",\n",
    "    \"Machine learning\",\n",
    "    \"Deep learning\",\n",
    "    \"Natural language\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different temperatures\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for temp in [0.1, 0.7, 1.0]:\n",
    "        generated = inference.generate(\n",
    "            prompt, \n",
    "            max_new_tokens=30, \n",
    "            temperature=temp\n",
    "        )\n",
    "        print(f\"Temp {temp}: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention patterns (simplified)\n",
    "model.eval()\n",
    "sample_text = \"The quick brown fox\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get embeddings\n",
    "    embeddings = model.embedding(tokens_tensor)\n",
    "    \n",
    "    # Forward through first transformer block to get attention\n",
    "    x, attention_weights = model.transformer_blocks[0](embeddings)\n",
    "    \n",
    "print(f\"Input tokens: {tokens}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Sample attention weights (first head):\")\n",
    "print(attention_weights[0, 0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "\n",
    "# Parameter breakdown by layer type\n",
    "param_breakdown = {}\n",
    "for name, param in model.named_parameters():\n",
    "    layer_type = name.split('.')[0]\n",
    "    if layer_type not in param_breakdown:\n",
    "        param_breakdown[layer_type] = 0\n",
    "    param_breakdown[layer_type] += param.numel()\n",
    "\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for layer_type, count in sorted(param_breakdown.items()):\n",
    "    percentage = (count / total_params) * 100\n",
    "    print(f\"{layer_type}: {count:,} ({percentage:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}