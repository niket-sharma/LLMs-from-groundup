{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Small GPT Demo\n",
    "\n",
    "This notebook demonstrates how to use the Small GPT model for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "from models.gpt import create_small_gpt\n",
    "from training.dataset import prepare_data, create_dataloaders, SimpleTokenizer\n",
    "from training.trainer import GPTTrainer\n",
    "from utils.inference import GPTInference\n",
    "from utils.helpers import set_seed, print_model_info, get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "set_seed(42)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_prep",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sample_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text length: 5110 characters\n"
     ]
    }
   ],
   "source": [
    "# Create sample text data\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This sentence contains all letters of the alphabet.\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience.\n",
    "Natural language processing involves the interaction between computers and human language.\n",
    "Deep learning uses neural networks with multiple layers to model and understand complex patterns.\n",
    "Transformers have revolutionized the field of natural language processing with their attention mechanisms.\n",
    "\"\"\" * 10  # Repeat for more data\n",
    "\n",
    "# Save to file\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "with open('../data/demo_text.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prepare_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 34\n",
      "Training samples: 4024\n",
      "Validation samples: 958\n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "train_dataset, val_dataset, tokenizer = prepare_data(\n",
    "    '../data/demo_text.txt', \n",
    "    block_size=64, \n",
    "    train_split=0.8\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_dataset, val_dataset, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_creation",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'd_model': 128,\n",
    "    'n_heads': 4,\n",
    "    'n_layers': 3,\n",
    "    'd_ff': 512,\n",
    "    'max_seq_len': 128,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = create_small_gpt(config)\n",
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = GPTTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr=1e-3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train for a few epochs\n",
    "train_losses, val_losses = trainer.train(epochs=3)\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_losses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training and Validation Loss (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference object\n",
    "inference = GPTInference(model, tokenizer, device)\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The quick\",\n",
    "    \"Machine learning\",\n",
    "    \"Deep learning\",\n",
    "    \"Natural language\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different temperatures\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for temp in [0.1, 0.7, 1.0]:\n",
    "        generated = inference.generate(\n",
    "            prompt, \n",
    "            max_new_tokens=30, \n",
    "            temperature=temp\n",
    "        )\n",
    "        print(f\"Temp {temp}: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention patterns (simplified)\n",
    "model.eval()\n",
    "sample_text = \"The quick brown fox\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get embeddings\n",
    "    embeddings = model.embedding(tokens_tensor)\n",
    "    \n",
    "    # Forward through first transformer block to get attention\n",
    "    x, attention_weights = model.transformer_blocks[0](embeddings)\n",
    "    \n",
    "print(f\"Input tokens: {tokens}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Sample attention weights (first head):\")\n",
    "print(attention_weights[0, 0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "\n",
    "# Parameter breakdown by layer type\n",
    "param_breakdown = {}\n",
    "for name, param in model.named_parameters():\n",
    "    layer_type = name.split('.')[0]\n",
    "    if layer_type not in param_breakdown:\n",
    "        param_breakdown[layer_type] = 0\n",
    "    param_breakdown[layer_type] += param.numel()\n",
    "\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for layer_type, count in sorted(param_breakdown.items()):\n",
    "    percentage = (count / total_params) * 100\n",
    "    print(f\"{layer_type}: {count:,} ({percentage:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
